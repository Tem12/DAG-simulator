import sys
import pandas as pd
import argparse
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True,
                        help='Data file generated by simulation')
    parser.add_argument('--csv', required=False, action='store_true', dest='csv',
                        help='Output data in csv format for another process')
    parser.add_argument('--extra-info', required=False, action='store_true', dest='no_extra_info',
                        help='Script will output also a used seed, config and data paths')

    args = parser.parse_args()

    # ================= Read metadata =================
    read_config = False
    read_seed = False
    read_blocks = False
    read_block_size = False

    blocks = 0
    block_size = 0
    config = ''
    seed = 0

    if not os.path.exists(args.data):
        print('Specified path to data file does not exists', file=sys.stderr)
        exit(1)

    # Cannot directly replace 'data_' because it can be used as name/part of the name of some folder in specified path
    data_prefix_index = args.data.rfind('data_')

    if data_prefix_index == -1:
        print('Invalid data file name', file=sys.stderr)
        exit(1)

    if data_prefix_index == 0:
        # data_...
        metadata_path = args.data.replace('data_', 'metadata_', 1)
    else:
        # {path}/data_...
        metadata_path = '/metadata_'.join(args.data.rsplit('/data_', 1))

    # Replace file extension
    metadata_path = metadata_path[:len(metadata_path) - 3] + 'data'

    if not os.path.exists(metadata_path):
        print(
            f'Metadata file for {args.data} cannot be found', file=sys.stderr)
        exit(1)

    with open(metadata_path, 'r') as metadata_file:
        for row in metadata_file:
            value = row.rstrip('\n')
            if read_config and read_seed and read_blocks and read_block_size:
                break

            if value.startswith('cfg_path='):
                config = value[len('cfg_path='):]
                read_config = True
            elif value.startswith('seed='):
                seed = int(value[len('seed='):])
                read_seed = True
            elif value.startswith('blocks='):
                blocks = int(value[len('blocks='):])
                read_blocks = True
            elif value.startswith('block_size='):
                block_size = int(value[len('block_size='):])
                read_block_size = True

    if not read_config:
        print(f'Metadata does not contain config path', file=sys.stderr)
        exit(1)

    if not read_seed:
        print(f'Metadata does not contain seed', file=sys.stderr)
        exit(1)

    if not read_blocks:
        print(f'Metadata does not contain number of blocks', file=sys.stderr)
        exit(1)

    if not read_block_size:
        print(f'Metadata does not contain block size', file=sys.stderr)
        exit(1)

    # =================================================

    tx_count = blocks * block_size

    df = pd.read_csv(args.data)

    # Store id of transaction as key with number of occururences as value
    txs = {}

    duplicate_tx_count = 0

    count_1 = 0  # transactions that appears only once in all blocks
    count_2 = 0  # 2x transaction duplication
    count_3 = 0  # 3x transaction duplicition
    count_4 = 0  # 4x transaction duplication
    count_5 = 0  # 5x transaction duplication
    count_more = 0  # more that 5x transaction duplication

    # Iterate all transactions to get how many times each of them appears in blocks
    for i in range(0, tx_count):
        tx_id = df['TransactionID'][i]
        if tx_id in txs:
            if txs[tx_id] == 1:
                count_1 -= 1
                count_2 += 1
            elif txs[tx_id] == 2:
                count_2 -= 1
                count_3 += 1
            elif txs[tx_id] == 3:
                count_3 -= 1
                count_4 += 1
            elif txs[tx_id] == 4:
                count_4 -= 1
                count_5 += 1
            elif txs[tx_id] == 5:
                count_5 -= 1
                count_more += 1
            txs[tx_id] += 1
        else:
            txs[tx_id] = 1
            count_1 += 1

    duplicate_tx_count = tx_count - count_1 - count_2 - \
        count_3 - count_4 - count_5 - count_more

    if not args.csv:
        print(f'Data: {args.data}')
        print(f'Config: {config}')
        print(f'Seed: {seed}')

        # Pure duplication count is complement to transaction throughput
        print(
            f'Pure duplicated transactions: {duplicate_tx_count} ({round(duplicate_tx_count / tx_count * 100, 2)}%)')
        print(
            f'1 tx count (unique): {count_1} ({round(count_1 / tx_count * 100, 2)}%)')
        print(
            f'2 tx count: {count_2} ({round(count_2 / tx_count * 100, 2)}%)')
        print(
            f'3 tx count: {count_3} ({round(count_3 / tx_count * 100, 2)}%)')
        print(
            f'4 tx count: {count_4} ({round(count_4 / tx_count * 100, 2)}%)')
        print(
            f'5 tx count: {count_5} ({round(count_5 / tx_count * 100, 2)}%)')
        print(
            f'More count: {count_more} ({round(count_more / tx_count * 100, 2)}%)')
    else:
        # Output format: (each value is only in percentage format)
        # {data_path},{config_path},{seed},{duplication_rate},{1_tx_count},{2_tx_count},{3_tx_count},{4_tx_count},{5_tx_count},{more_tx_count}
        #
        # Example:
        # outputs/data_mining_topo.cfg_0000.csv,mining_topo.cfg,512,39.5916,27.696,26.4726,5.64,0.5622,0.0358,0.0018
        print(f'{args.data},{config},{seed},{round(duplicate_tx_count / tx_count * 100, 4)},{round(count_1 / tx_count * 100, 4)},{round(count_2 / tx_count * 100, 4)},{round(count_3 / tx_count * 100, 4)},{round(count_4 / tx_count * 100, 4)},{round(count_5 / tx_count * 100, 4)},{round(count_more / tx_count * 100, 4)}')

    return 0


if __name__ == '__main__':
    main()
