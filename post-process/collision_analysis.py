import sys
import pandas as pd
import argparse
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True,
                        help='Data file generated by simulation')
    parser.add_argument('--csv', required=False, action='store_true', dest='csv',
                        help='Output data in csv format for another process')

    args = parser.parse_args()

    # ================= Read metadata =================
    read_config = False
    read_blocks = False
    read_block_size = False

    blocks = 0
    block_size = 0
    config = ''

    if not os.path.exists(args.data):
        print('Specified path to data file does not exists', file=sys.stderr)
        exit(1)

    # Cannot directly replace 'data_' because it can be used as name/part of the name of some folder in specified path
    data_prefix_index = args.data.rfind('data_')

    if data_prefix_index == -1:
        print('Invalid data file name', file=sys.stderr)
        exit(1)

    if data_prefix_index == 0:
        # data_...
        metadata_path = args.data.replace('data_', 'metadata_', 1)
    else:
        # {path}/data_...
        metadata_path = '/metadata_'.join(args.data.rsplit('/data_', 1))

    # Replace file extension
    metadata_path = metadata_path[:len(metadata_path) - 3] + 'data'

    if not os.path.exists(metadata_path):
        print(
            f'Metadata file for {args.data} cannot be found', file=sys.stderr)
        exit(1)

    with open(metadata_path, 'r') as metadata_file:
        for row in metadata_file:
            value = row.rstrip('\n')
            if read_config and read_blocks and read_block_size:
                break

            if value.startswith('cfg_path='):
                config = value[len('cfg_path='):]
                read_config = True
            elif value.startswith('blocks='):
                blocks = int(value[len('blocks='):])
                read_blocks = True
            elif value.startswith('block_size='):
                block_size = int(value[len('block_size='):])
                read_block_size = True

    if not read_config:
        print(f'Metadata does not contain config path', file=sys.stderr)
        exit(1)

    if not read_blocks:
        print(f'Metadata does not contain number of blocks', file=sys.stderr)
        exit(1)

    if not read_block_size:
        print(f'Metadata does not contain block size', file=sys.stderr)
        exit(1)

    # =================================================

    total_count = blocks * block_size

    df = pd.read_csv(args.data)

    txs = {}

    count_0 = 0  # transactions that have not been processed during simulation
    count_1 = 0  # transactions that appears only once in all blocks
    count_2 = 0  # 2x transaction duplication
    count_3 = 0  # 3x transaction duplicition
    count_4 = 0  # 4x transaction duplication
    count_5 = 0  # 5x transaction duplication
    count_more = 0  # more that 5x transaction duplication

    # Iterate all transactions to get how many times each of them appears in blocks
    for i in range(0, total_count):
        tx_id = df['TransactionID'][i]
        if tx_id in txs:
            if txs[tx_id] == 1:
                count_1 -= 1
                count_2 += 1
            elif txs[tx_id] == 2:
                count_2 -= 1
                count_3 += 1
            elif txs[tx_id] == 3:
                count_3 -= 1
                count_4 += 1
            elif txs[tx_id] == 4:
                count_4 -= 1
                count_5 += 1
            elif txs[tx_id] == 5:
                count_5 -= 1
                count_more += 1
            txs[tx_id] += 1
        else:
            txs[tx_id] = 1
            count_1 += 1

    count_0 = total_count - count_1 - count_2 - \
        count_3 - count_4 - count_5 - count_more

    if not args.csv:
        print(f'Data: {args.data}')
        print(f'Config: {config}')

        print(
            f'0 tx count (not processed): {count_0} ({round(count_0 / total_count * 100, 2)}%)')
        print(
            f'1 tx count (unique): {count_1} ({round(count_1 / total_count * 100, 2)}%)')
        print(
            f'2 tx count: {count_2} ({round(count_2 / total_count * 100, 2)}%)')
        print(
            f'3 tx count: {count_3} ({round(count_3 / total_count * 100, 2)}%)')
        print(
            f'4 tx count: {count_4} ({round(count_4 / total_count * 100, 2)}%)')
        print(
            f'5 tx count: {count_5} ({round(count_5 / total_count * 100, 2)}%)')
        print(
            f'More count: {count_more} ({round(count_more / total_count * 100, 2)}%)')
        print(
            f'Duplicates (2 tx count or more): {round((total_count - count_0 - count_1) / total_count * 100, 2)}%')
        print(
            f'Duplicates with 0 count: {round((total_count - count_1) / total_count * 100, 2)}%')
    else:
        # Output format: (each value is only in percentage format)
        # {data_path},{config_path},{0_tx_count},{1_tx_count},{2_tx_count},{3_tx_count},{4_tx_count},{5_tx_count},{more_tx_count},{duplicates_2tx_or_more},{duplicates_0_count}
        #
        # Example:
        # outputs/data_mining_topo.cfg_0000.csv,mining_topo.cfg,39.5916,27.696,26.4726,5.64,0.5622,0.0358,0.0018,32.7124,72.3
        print(f'{args.data},{config},{round(count_0 / total_count * 100, 4)},{round(count_1 / total_count * 100, 4)},{round(count_2 / total_count * 100, 4)},{round(count_3 / total_count * 100, 4)},{round(count_4 / total_count * 100, 4)},{round(count_5 / total_count * 100, 4)},{round(count_more / total_count * 100, 4)},{round((total_count - count_0 - count_1) / total_count * 100, 4)},{round((total_count - count_1) / total_count * 100, 2)}')

    return 0


if __name__ == "__main__":
    main()
